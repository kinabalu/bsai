= Introduction
:chapter: 1

Welcome to Beginning Spring AI!

Spring AI refers to the Spring Framework's suite of libraries meant to help programmers use some of the more common artificial intelligence technologies in the marketplace.

In this book, we're going to take a quick tour through some of the libraries and their features, not so much to provide an in-depth reference on every feature, but to communicate enough information such that readers can see the potential of the technology, and use _most_ of what they'd care about.

NOTE: This chapter has no code.
It's all foundational knowledge about artificial intelligence and definitions and scopes ("oh my!"), and talks about what technologies the book covers and how it can be thought of as a tool.
If you want to get cracking with code, you should feel free to skip to Chapter 2 - but this chapter's pretty useful, and you'll eventually want to read it.

== AI is all the rage

It's hard to read anything on the Internet today without being at least tangentially aware of AI.
It really doesn't matter what industry or topic is involved; AI is used to write prompts on sites like Quora, it's used to design quick and easy visual arts (even music videos), or music - there're songs written fully by AI, some possibly quite respectable.

In programming, most IDEs have integrations available with AIs that help suggest code or revisions to existing code; if you know how to create queries well enough, you can get an AI to generate nearly all of a working application, a capability that's resulted in some managers wondering whether they even _need_ human engineersfootnote:[Spoiler alert: yes, the managers do need human engineers. We'll get to why soon, some in this chapter and some in Chapter 6.].

Authors, too, have felt the impact of AI "writing." AI can not only help correct text - with many writing aids that will suggest grammar, spelling, even evocative text - but AI can even write stories, and if the prompts used are detailed enough, some of the stories might pass for having written by a human author.

NOTE: It's worth noting for the record: this book has indeed been impacted by AI, and we'll point out why and how later.
We promise.
At least, that's what the AI said to say.
Or did it write it?
Sometimes it's hard to tellfootnote:[This is intended to be humor. If an AI generates any actual content of note in this book, we'll be pointing it out, even if it's not obvious. With that said, when we say AI is used in many writing tools, we mean it; a lot of our grammar was checked and occasionally fixed by AI. It's also worth noting that not even _one_ of the footnotes was suggested by an AI.].

The effect can be chilling: are we replaceable by a machine after all?
If a machine can take a specification for an application, or a song, or a story, and generate something that's approaching the quality of a human - or better, in some ways - then wh do we need humans?

There are a number of good answers, and some bad ones, too.

== What is AI, really?

First, let's consider what AI really is, and limit the scope of what we're talking about, because "AI" - artificial intelligence - is a term that has _many_ applications, and we're not going to go into all (or even "most") of them.

Here's how an AI summarized artificial intelligence:

[source,text]
----
Artificial Intelligence (AI) refers to the development
of computer systems that can perform tasks that normally
require human intelligence, such as learning, problem-solving,
and understanding language. At its core, AI uses mathematical
models to make decisions and predictionsâ€”ranging from
simpler approaches like Markov chains, which predict
sequences of events, to more complex systems like recommender
algorithms that suggest movies or products based on user
preferences. With advancements in AI, more sophisticated models
like Large Language Models (LLMs) have emerged, allowing
systems to understand and generate human-like text. These
models can be integrated into applications, such as those
built with Spring AI, to automate and enhance various
tasks across industries.
----

Not bad, `ChatGPT4o`!
This is a good example of a "good use" of AI: the prompt used here asked for a single paragraph that covered the general and historical use of AI, leading up to the technology most commonly easily exploited today, the Large Language Model, or LLM.
(`ChatGPT4o` refers to a specific LLM, hosted by OpenAI.)

In the end, it's all literally math, as the paragraph suggested: an input is reduced to a series of numbers, and the AI ... does something with those numbers (what it does depends on the type of AI and its purpose).
It then generates _more_ numbers, and those numbers map to an output - and that applies to nearly every AI model, including the one that generated the paragraph.

Some of the earliest uses of AI models - early enough that some might scoff at the reference to them as "artificial intelligence" - were for predicting weather or making simple deductions based on probabilities.

One simple modeling type is the Markov chain, for example, which is a model that generates a likely outcome based on a string of inputs.
This is actually fairly useful in predicting weather with few variables, and that's actually how it worked.

Consider: if it's raining today, what's the likelihood of it raining _tomorrow?_ If it was raining _yesterday_, does that change the likelihood of rain at all?
What if it's been raining heavily for a week?
Consider also the history of the region being predicted; if it rains every day in June, for example, and it's June, _and_ it's been raining every day so far, can rain tomorrow be predicted?
A Markov chain reduces the history to a series of tokens (which might combine the day of the year, the region, and the weather for that day) and have a set of outcomes that says "95% of the time, when the prior condition looks like _this_, the next day will look like _that_."

Markov chains are also useful for generating text; the ELIZA programfootnote:[An example of ELIZA can be found at `https://web.njit.edu/~ronkowit/eliza.html` . Try it! It's fun! beware: it might make you think of your mother.] emulates a therapist similar to Carl Rogers, by taking significant parts of what you say to it and repeating those parts back to you, in a few different combinations.

Markov chains work on Bayesian probabilities internally; Bayesian algorithms found a lot of use in early attempts to filter email spam, to quite a bit of success, although obviously spammers have found ways to avoid easy detection.

If probabilities don't sound like "artificial intelligence," well, that's fine, but it's generally unfair.
Probabilities rule much of our lives; when we go for a drive, we look at our fuel and calculate how likely it is that we'll need to purchase more, or when, or whether we'll need to take an umbrella or a coat, after all.

AI also has a sort of "magic appeal" in how difficult it can be to understand what's going on behind an invocation.
In the end, it's all math, but ... _what_ math?
And if it's math, well, do we _need_ AI?

The answer is, as you might suspect, "it depends." "What math?" is answered by another question: "What are you trying to do?" An LLM does a lot of stemming and tokenization to reduce a prompt to a series of numbers, which generates some amusing outcomes sometimesfootnote:[A few weeks ago as of this writing, it was a meme about AI that the LLMs couldn't tell how many occurrences of the letter "R" were in the word "strawberry." To us, it's obviously three; to the LLM, however, it was counting based on the tokenized version of the word, which had two Rs, not three, and it ended up looking hopelessly confused, even when corrected.].
A Markov chain gauged towards weather would have _no_ sensible output to an input that asked what `1+1` was - it's simply too sensitive to its problem domain.

It's also important to keep in mind that the reason we have so many models and definitions is because there are so many applications, some appropriate and some not.

It's an aphorism that "when all you have is a hammer, everything looks like a nail." As of this writing, this feels like an accusation; it's rather common to see "can we throw an AI at it?" as a response to problems that, honestly, don't need all that much computation power.

A logistics company might throw artificial intelligence at predicting whether a given package will be late or not - when the answer can be predicted almost as well by simply asking what the weather is like at the destination of the package.
This is a poor application of the resources and power of AI.

That's not to say that you shouldn't use artificial intelligence, at all: it's more an observation that AI is a tool, and like most tools, it has appropriate uses and can be overused, and given the current enthusiasm around a term that few seem to understand, is *often* overused.

== The Scope of This Book

This book primarily focuses on Large Language Models and how to access them with the Spring Framework.

Large language models use a number of techniques to reduce textual prompts to mathematical models, and apply *other* mathematical models to generate responses, also generally textual in nature, although chapters 4 and 5 will show you how to feed audio and visual information into an LLM, and get audio and visual information *out*.

== How can LLMs be used?

LLMs are effectively information blenders; you give them a filter (the prompt) and they generate a probabilistic outcome based on the information on which they were trained.

Therefore, selecting a model can be of critical importance.
(You wouldn't want to use a model trained primarily on fantasy literature to make medical conclusionsfootnote:[You probably wouldn't want to rely on an LLM's medical conclusion even if it were trained on medical data, actually; you'd want an actual competent doctor to make decisions, possibly factoring in observations by an LLM.])

One way to think about the output of an LLM is as if it were selected at random based on what other people _might_ have said, as if the LLM were to take all of the possible answers to your prompt, stir them together and pick elements at random, and then present the result in a cohesive manner.

This is why stories written by an AI tend to be faintly familiar: they are!
They're taking common elements of storytelling and replacing bits as they go, and the result can feel original at times while feeling horribly derivative at other times.
That doesn't mean the story isn't worth telling - most stories in human history have a similar set of concepts at their hearts, as Joseph Campbellfootnote:[Joseph Campbell wrote a book in 1949 called "The Hero with a Thousand Faces," that described a common set of concepts in human mythology, often summed up as the "Hero's Journey." See `https://www.amazon.com/Thousand-Faces-Collected-Joseph-Campbell/dp/1577315936` for more.] might have told you - but it also isn't the same as coming up with "original content."

But with this observation - that LLMs are stirring up knowledge we already had in possibly unexpected ways to come up with content - it's worth saying that this is _useful_.
Sometimes things we want to know are "hiding in plain sight," obscured by tradition and expectation, and an LLM doesn't have the burden of prior knowledge; it can see common patterns that humans can overlook, and without models being specifically limited in what they can observe, an LLM is able to point out that the emperor's not wearing any clothes fairly easily.

== How do you choose an LLM?

That's a good question!
As with others, the answer is "it depends on what you want," combined with what you want to spend and the cost of using a given LLM.

There are a lot of choices: ChatGPT (from OpenAI), Meta (from Facebook), Grok (from X), Amazon Bedrock, Claude, and Ollama, and that's just a *few* of the options.
Most of them use a similar API endpoint (after all, they do have a pretty common usage pattern), but their capabilities aren't quite the same; Ollama, for example, as of this writing doesn't support audio or image generation in and of itself, while ChatGPT certainly does.

This is actually why you'd want to use Spring AI: it abstracts much of the low-level APIs into a common framework.
There are areas in which you *are* coding to a specific LLM, particularly when setting the options for how it generates content, but that's *usually* it, and those features can often be set by configuration rather than being set specifically in code.

As far as choosing an LLM: this book primarily focuses on using ChatGPT, because it was one of the first major vendors for AI services using an LLM, and it's remarkably sufficient for a general-purpose AI without being absurdly expensive.
Ollama has the benefit of running locally, if you have a sufficient GPU; it can run without a GPU, in CPU mode, but tends to result in _very_ slow response times.

With that said, the main way to make a decision about which LLM to use is to _try them_ for your purpose.

Work out your application's purpose, write tests that submit to your AI of choice, and see how it performs against other LLMs, and balance the response time and cost against your needs.

== How much does it actually cost?

The popular LLMs (apart from Ollama) have a subscription model, where you typically spend a set sum per month to get a certain amount of allocated processing time.
For most users, the base subscription price will be more than enough; the process of writing this book, for example, including tests and development of them, works out to less than one US dollar, although there are factors that contribute to this amountfootnote:[Most of the models used in this book are lower-performance, and thus lower cost, models.]

Factors that play in are the usage costs for the models being chosen, with higher-capability models having higher costs, and token counts.
Typically, token costs for a given model are how expenses are calculated.

This book uses a lot of very short AI prompts, generally, so the token counts for the entire book, added together, work out to probably under a thousand tokensfootnote:[This is a guess. We could calculate it, because interactions with an LLM include token counts in the response metadata, but given that the total cost for the book would have been under a dollar if the subscription cost didn't cover the required resources, it's just not worth the effort. Your mileage may vary.].
If you run the entire book's tests over and over again, that adds up, but it's still not a lot.

If you're doing a lot of detailed analysis covering a lot of data, your token counts will be higher, and you might run into costs associated with analysis; Chapter 3 covers some ways to mitigate this, but in the end, if you need a certain number of tokens to achieve a task, you... need a certain number of tokens to achieve a task, and your selection of a model and provider will be balanced against your requirements.

The short version of all of that: expect a relatively minor subscription cost, and watch your typical usage to try to predict whether you need additional capacity or not.
If you do need more capacity, consider whether you have the resources to run Ollama locally (meaning that you have a decent GPU and RAM, and a fast disk), and _try it_.

The advantage of external AI providers is that they have massive server farms to throw at tasks, meaning that you can work with larger models and expect faster response times, with more features; the disadvantages of external providers is that they can see your prompts (and how that's used is up to the provider; read the fine print!) and you have to pay for their services.

== What This Book Isn't

This book is going to cover a lot of code, of course, being a book about Spring AI.
However, it presumes you know Java to some degree, and have some familiarity with the Spring Framework (and Spring Boot) already.

It requires you to have Java and Maven installed, although handy links will be provided just in case you don'tfootnote:[Your authors have no idea why books on programming have to walk through basic things like "installing your language of choice," but if you don't have some of that, the technical reviewers whine about it.].

This book does *not* require an IDE.
You'll want one, we think, but ... which one?
We don't know, and don't care.
You can use a text editor, if that's what you desire, or IDEA, or Eclipse, or NetBeans, or Visual Studio Code; we offer these names as they occurred to us to write, not as an indication of preference in any way.

The book also generally focuses on tests as a way to demonstrate technique.
There are a few places where there's an application to execute (particularly in Chapter 4, which provides a web-based application to convert text to speech) but the _primary_ demonstration is in setting expectations of output given a specific set of inputs, and validation of that output.

When the code compiles and the tests pass, the code works.
Otherwise, there's not a lot to demonstrate, so there aren't a lot of screenshots to look for.
(Given the nature of probabilistic outputs from LLMs, though, there are places where you might be expected to look at a generated string to make sure it fits your expectations, although we're generally trying to avoid this.)

It's also not a book that's exhaustively going to cover every AI technique - or even every possibility of how to work with a given LLM.
It's focused on large language models, and other techniques are more advanced topics than the topics at which this book is targetedfootnote:[Honest truth: your author considered having an AI rewrite that sentence.].
LLMs have different capabilities and settings; part of why we chose ChatGPT was because ChatGPT covers the features that most people want, and other LLMs may or may not provide the same set of features, but few other LLMs provide features ChatGPT does _not_.
Readers who wish to use alternatives should be able to fine-tune the example code for their specific AI implementation without too much effort.
(And if it takes a lot of effort, feel free to reach out to your authors; we're very interested in helping the industry move forward!)

== Next Steps

In our next chapter, we're going to walk through setting up a project that includes calling ChatGPT through Spring AIfootnote:[It would not be difficult to use any other LLM provider, but again, this book uses ChatGPT, because it's very common, well known, and very predictable - and it definitely provides all of the services the book covers.].
