= Working with Audio
:chapter: 4

== Generating and Processing Audio

Since the early 19th century, humans have expressed the idea of a mechanical device capable of speech in fiction, through authors like Jules Verne and later H.G. Wells.
By the mid 20th century in fiction we had moved from "Ulla, Ulla" to machines that were capable of talking back (text to speech) and understanding speech (transcription).
Today we are surrounded by mechanical devices that have made the vision of Roddenberry and others come to life whether prefaced with "Computer", "Siri" or "Alexa" the result is a device that understands the basics of what you ask it like requests for weather or a piece of music.

Today, we can reasonably speak something as futuristic as "Computer, write me a chapter on using Spring AI to generate and process audio" and what would be returned might just be a good first startfootnote:[As we mentioned in Chapter 1, unless we mention it the text of this book was written by one of the human authors.].
Since we're still actually writing the text here, let's turn our attention toward what we can achieve using AI with the spoken word.
One can imagine transcribing video or audio to written text as part of a service offering.
Written text can be spoken aloud using this module for the vision-impaired and for those engaged in activities where reading isn't an option or as a response to commands in a system like Siri.

Spring AI has two avenues for working with audio.
The first is transcribing text from a supplied audio file and the second is generating an audio file with natural sounding speech.
In both instances the integration done today with Spring AI uses OpenAI.
Hopefully in the near future we'll see integrations with other providers like Elevenlabs or Google's Gemini.

In this chapter we will walk through the two simple APIs for processing audio and text and real world examples of how you can use both.

Let's dive in.

=== The AI spoken word

Text-to-speech technology has a long history, dating back to the 1960s when Bell Labs used an IBM 7094 computer to synthesize speech for 'Daisy Bell,' even influencing Arthur C. Clarke's depiction of HAL9000 in '2001: A Space Odyssey.'

Text-to-speech synthesis is a fascinating programming challenge because it demands bridging the gap between abstract language and human perception.
The models must grapple with not just grammar, but also the nuances of pronunciation, intonation, and rhythm that make speech sound natural.

This shift towards AI-powered solutions brings a host of benefits.
Programmers now have access to a wider range of voices, more natural-sounding speech, and simpler integration processes.
Service providers handle the heavy lifting, freeing developers to focus on creative applications and pushing the boundaries of flexible models.

The first task we'll take on is to create our directory structure for this chapter.
This can be created in the "project directory" as in previous chapters with the following command, if you're running a POSIX shell like `bash` or `zsh`:

.Listing {chapter}-{counter:listing}: Creating the project directory structure in POSIX
[source,shell]
----
mkdir -p src/{main,test}/{java,resources}
----

This is _this chapter's_ project file, and thus it's in a directory _under_ the top level directory, called `chapter04`, and the file is named `pom.xml`.

.Listing {chapter}-{counter:listing}: `chapter04/pom.xml`
[source,xml]
----
include::../code/chapter04/pom.xml[lines="1..20,26..-1"]
----

The first thing we need to handle as we did in previous chapters is the application configuration. Thanks to the magic of the library `spring-ai-spring-boot-autoconfigure` which has the code for auto configuration it will pick up the configuration for working with audio in your application configuration. The most important property to ensure is set is the `spring.ai.openai.api-key`.

.Listing {chapter}-{counter:listing}: `chapter04/src/main/java/ch04/Ch04Configuration.java`
[source,java]
----
include::../code/chapter04/src/main/java/ch04/Ch04Configuration.java[lines="1..2,8..16,58..-1"]
----

A `OpenAiAudioApi` has three main request modules that it enables for working with audio.
The first we'll work with is the `SpeechRequest` which may just be the text we want to create an audio file but can also accept several other options which we'll go over below.
As with other modules in Spring AI they can be controlled at the point of the request, in code, or from the application properties.

[cols="1,2"]
|===
| Property name
| Description

| `spring.ai.openai.api-key`
| The API key to be used by the application. This isn't utilized by `OpenAiAudioApi` but we'll continue with this convention here.

| `spring.ai.openai.audio.speech.options.model`
| The name of the model to use. `tts-1` and `tts-1-hd` are both available.

| `spring.ai.openai.audio.speech.options.voice`
| The voice to use for the TTS output. Available options are: `alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer`.

| `spring.ai.openai.audio.speech.options.response-format`
| The format of the audio output, supported formats are `mp3`, `opus`, `aac`, `flac`, `wav`, and `pcm`.

| `spring.ai.openai.audio.speech.options.speed`
| The speed of the voice synthesis between `0.0` and `1.0`.
|===

For this chapter we're going to focus on tests but also we will have an opportunity to explore a real world case where we can use the Spring AI Audio API with Spring MVC.

We need to at least provide the API key to our chapter's code, and we want the model to be as deterministic as possible for now, so here's our `application.properties`.
This is all very test-centric, so we're going to place it in `chapter02/src/test/resources`.
Note the use of `spring.config.import`, which allows us to load our `.env` file's values for internal use.

.Listing {chapter}-{counter:listing}: `chapter04/src/test/resources/application.properties`
[source,properties]
----
include::../code/chapter04/src/test/resources/application.properties[]
----

Our first test case is going to be based on the Daisy Bell songfootnote:[As mentioned in the rather informative article, "The IBM 7094 is the First Computer to Sing,` at `https://www.historyofinformation.com/detail.php?entryid=4445` - well worth reading.] that we referenced at the beginning of the chapter.
Our test is simple; We're going to pass the lyrics to the song "Daisy Bell" to service `TextToSpeechService` and call the `processText` method.
We'll take the response and assert that it is non-null as the actual response would be tough to verify in a test.
(However, as we'll demonstrate, we can save it to local storage, where you can play it yourself and hear whether the AI actually fulfilled our request.)

.Listing {chapter}-{counter:listing}: `chapter04/src/test/java/ch04/SpeechTTSTest.java`
[source,java]
----
include::../code/chapter04/src/test/java/ch04/SpeechTTSTest.java[]
----

If you are interested in the output as part of the test you could write the mp3 to disk.

.Listing {chapter}-{counter:listing}: Write MP3 to disk
[source,java]
----
Files.write(Paths.get("./daisybell.mp3"), responseAsBytes);
----

This test is simple enough, but it obviously won't run because we don't have an implementation of the `TextToSpeechService`.
The service has two public methods in it, `processText(String)` and `processText(String, OpenAiAudioSpeechOptions.Builder)`, which both return a `byte[]`.

We have a `SpeechModel` available which we'll use Spring's dependency injection to reference in our `TextToSpeechService`.

Issuing a call to OpenAI, we build a `SpeechPrompt` and pass the text we'd like to use to generate spoken word output.
If we don't pass an `OpenAiAudioSpeechOptions` object into the `processText` method it will by default use the options from `application.properties` if they're specified.

Once we have a prompt, we issue a call to the API, whether blocking or streaming; in this case, we don't care about streaming, so we use `call()` to get the response specification.

Once we have the response specification, we can get the recording by using the `getResult().getOutput()` method which returns a byte array.
If we wanted to, we could get metadata about the call like the number of tokens consumed on our API key and a few other interesting elements.

This sounds pretty straightforward, so let's see what the actual Java class looks like.

.Listing {chapter}-{counter:listing}: `chapter04/src/main/java/ch04/service/TextToSpeechService.java`
[source,java]
----
include::../code/chapter04/src/main/java/ch04/service/TextToSpeechService.java[]
----

We've got something to test now, so we can run this chapter's test suite by using Maven:

[source,shell]
----
mvn -am -pl chapter04 clean test
----

If everything is working properly it should complete with `SUCCESS`.
If you chose to add the line for writing the file output to disk you can open up the resulting MP3 file and have a listen.
This test will sound more like spoken word than singing as there is no "singing" option within the OpenAI API; the result is a particularly passionless rendering of a silly love song, and might be worth listening to only for humor's sake.

Let's say we wanted to pass some extra options to make it more like a song.
If we pass a custom `OpenAiAudioSpeechOptions` to a custom test we can do so like this.

.Listing {chapter}-{counter:listing}: `chapter04/src/test/java/ch04/SpeechTTSTestWithOptions.java`
[source,java]
----
include::../code/chapter04/src/test/java/ch04/SpeechTTSTestWithOptions.java[]
----

This test generates an MP3 file on disk which, with the given options, aims to sound more like a song.
However, itâ€™s safe to say that vocalists like Celine Dion, Adele, and Janis Joplin have nothing to worry about from OpenAI just yet.
The Whisper model we are using has been trained on the spoken word in the various supported languages.
In order to provide a method of "text-to-sing" we'd need to model the aspects of melody, pitch, rhythm and the various other musical elements involved in realistic sounding singing.

There are general purpose TTS systems on the market today (Tacotron 2, WaveNet, and FastSpeech) that can be adapted by training the model on singing data including the music and patterns involved.
It would also be important to provide the actual melody in the form of perhaps a MIDI file to teach the model the song it's singing.

=== Transcription

The other primary method available in these modules is the ability of Spring AI to integrate with a provider like OpenAI to transcribe audio of spoken words to text.
Our tests previously have been using the lyrics to "Daisy Bell", and we will continue that here.
A search on Wikipedia gives us a FLAC file of the original recording.
If you'd like to download the file using the command line tool `curl` you can see how to do that below, or opening a browser and saving the file to disk from the URL: `https://bit.ly/daisy_bell_dectalk`.

.Listing {chapter}-{counter:listing}: Download the Daisy Bell FLAC file
[source,bash]
----
curl -O 'https://bit.ly/daisy_bell_dectalk'
----

Our transcription test will be based on this song, after downloading you can place the file in the chapter's `src/test/resources` directory.
The test is fairly simple, we'll read the file from the classpath and pass the audio content to the `TranscribeService` and call the single method `transcribeAudio`.
The response we receive back will be verified using an assertion of a phrase from the original lyrics, because transcription is not an exact art.
Even when the words match (as they usually will), the punctuation can be different, so an exact match of the output is unlikely.
As a result, we're going to search for a phrase we do indeed fully expect to be in the output (the closing line) and leave it at that.

Let's take a look at our test.

.Listing {chapter}-{counter:listing}: `chapter04/src/test/java/ch04/TranscribeTest.java`
[source,java]
----
include::../code/chapter04/src/test/java/ch04/TranscribeTest.java[]
----

We can't quite run this yet since the `TranscribeService` doesn't exist.
If a reader looks at the assertion we might also notice that what we're verifying is not exactly the original lyrics.
Listening to the recording we could definitely see how OpenAI would mistake "do" with "too".
Now let's take a look at the `TranscribeService` and see how we make the transcription magic happen.

.Listing {chapter}-{counter:listing}: `chapter04/src/main/java/ch04/service/TranscribeService.java`
[source,java]
----
include::../code/chapter04/src/main/java/ch04/service/TranscribeService.java[]
----

We have a `OpenAiAudioTranscriptionModel` available which we'll use Spring's dependency injection to reference in our `TranscribeService`.

Issuing a call to OpenAI we build an `AudioTranscriptionPrompt` and pass the text we'd like to use to generate the output.
If we don't pass an `OpenAiAudioTranscriptionOptions` object into the `transcribeAudio` method it will by default use the options from `application.properties` if specified.

Once we have a prompt, we issue a call to the API, whether blocking or streaming; in this case, we don't care about streaming, so we use `call()` to get the response specification.

Once we have the response specification, we can get the recording by using the `getResult().getOutput()` method which returns a String.
As with other calls we could choose to look at the metadata which is available from the call to `getResult()` and for our test and usage we're focused only on the output.

We've got something to test now, so we can run this chapter's test suite by using Maven.

[source,shell]
----
mvn -am -pl chapter04 clean test
----

If the stars align and the AI hasn't done any hallucinating during your test you should see `SUCCESS` in the test.

The neat thing to consider here is that the phrase is actually rendered pretty well; we expect it to render the _number_ two (as in "a bicycle built for two") and not the homonym "too" - the AI model is usually going to infer the proper words based on the phrases in the song, so while the punctuation and capitalization may be slightly variable, _in general_ it's rendering a quite recognizable transcription of the input, _without_ necessarily recognizing the song itself.

[cols="1,2"]
|===
| Property | Description

| spring.ai.openai.audio.transcription.options.model
| ID of the model to use. Only whisper-1 (which is powered by our open source Whisper V2 model) is currently available.

| spring.ai.openai.audio.transcription.options.response-format
| The format of the transcript output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`.

| spring.ai.openai.audio.transcription.options.prompt
| An optional text to guide the modelâ€™s style or continue a previous audio segment. The prompt should match the audio language.

| spring.ai.openai.audio.transcription.options.language
| The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency.

| spring.ai.openai.audio.transcription.options.temperature
| The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.

| spring.ai.openai.audio.transcription.options.timestamp_granularities
| The timestamp granularities to populate for this transcription. response_format must be set verbose_json to use timestamp granularities. Either or both of these options are supported: word, or segment. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.
|===

=== REST Example

A real world example might be an endpoint using Spring MVC which processes text or audio files.
Our next step will to be setting up two simple POST endpoints one which accepts a String of text and the other which accepts an audio file.

Calling this endpoint might look something like:

.Listing {chapter}-{counter:listing}: Calling the TTS endpoint with Daisy Bell lyrics
[source,shell]
----
curl -X POST http://localhost:8080/api/tts \
-H "Content-Type: application/json" \
--output test.mp3 \
--data-binary @- << EOF
{"text": "Daisy, Daisy, \nGive me your answer, do! \nI'm half crazy, \nAll for the love of you! \nIt won't be a stylish marriage, \nI can't afford a carriage, \nBut you'll look sweet upon the seat \nOf a bicycle built for two!"}
EOF
----

The above is passing a JSON object with a key of `text` and the value as the string we're looking to pass in.
For transcribing an audio file containing spoken words we can imagine the curl command to look like this:

.Listing {chapter}-{counter:listing}: Sending the Daisy Bell audio file to transcribe endpoint
[source,shell]
----
curl -X POST http://localhost:8080/api/transcribe \
-F "file=@./src/test/resources/Daisy_Bell_sung_by_DECtalk.flac"
----

Running this from the chapter root should pick up the file that was downloaded earlier in the chapter and imagine it would return back the transcribed text.

We'll start our implementation by creating a simple data class called `TextToSpeechRequest` which we can use for our endpoint `/api/tts`.

.Listing {chapter}-{counter:listing}: `chapter04/src/main/java/ch04/model/TextToSpeechRequest.java`
[source,java]
----
include::../code/chapter04/src/main/java/ch04/model/TextToSpeechRequest.java[]
----

After this our implementation will be fairly straightforward using Spring MVC.
We will use dependency injection to insert our two services: `TextToSpeechService` and `TranscribeService`.
Our two handlers will accept `POST` requests with either a `@RequestBody` of `TextToSpeechRequest` as we saw above for processing the text to speech, or a `MultipartFile` for transcription.

.Listing {chapter}-{counter:listing}: `chapter04/src/main/java/ch04/handler/AudioTextController.java`
[source,java]
----
include::../code/chapter04/src/main/java/ch04/handler/AudioTextController.java[]
----

We can run this from the chapter root using Maven.

[source,shell]
----
mvn spring-boot:run
----

In a separate terminal session we can use the curl commands above and see the API responses sent back similarly as with the tests.

=== A simple voice assistant

Siri was the first voice-activated assistant available in a consumer device in early 2010. Likely due to it being the first and the compute power required it was mired in problems as it had to process speech using cloud servers and thus required a stable and fast internet connection.
Fast forward to today and thanks to the advancements in natural language processing and on-device hardware we've moved to a context aware system that understands your speech and most requests you give it.

We are going to use what we've learned in this chapter and borrow code from the previous to create a simple virtual assistant.
This is only a simple example and if you were building an actual voice assistant other choices would be more appropriate from a technology perspective.

==== The voice assistant task

This task will create a new service called `VirtualAssistantService` which will make calls to the other two services we built in this chapter.
In addition we will add integration with the `UpdateChatService` from chapter 3 to act on our lights via the chosen LLM.

We're going to take a slight shortcut for the audio command portion in our example and use the `TextToSpeechService` for creating the audio.
If you would like to record the audio command yourself we will call out how you can modify the `VoiceAssistantServiceTest` to pick up your file.
If we think about this logically using the code we have today we will be doing the following actions:

1. Generate the audio file with the command using the `TextToSpeechService`.
2. Take that audio file (or one you record yourself) and pass it to the `TranscribeService` to include in our prompt to `UpdateChatService`.
3. We'll pass the text from the `TranscribeService` to the `converse` method of `UpdateChatService` which calls the appropriate function based on the audio command.
4. Finally we'll pass the text output of the `converse` method to the `TextToSpeechService` and save the resulting spoken word audio to disk.

With that, our first task is including chapter 3 in our maven configuration files, This will let us re-use the services and methods involved in our ability to manage the real world we created in the previous chapter of light management using the LLM.
Include the following block anywhere in the `<dependencies>` block.

.Listing {chapter}-{counter:listing}: `chapter04/pom.xml`
[source,xml]
----
include::../code/chapter04/pom.xml[lines="21..26"]
----

Our next task is to modify our `@Configuration` to include the `UpdateChatService` along with it's dependencies.
We are also creating some mocked light status that we can act on with our voice assistant.
The block below can be included anywhere within the `Ch04Configuration` file:

.Listing {chapter}-{counter:listing}: `chapter04/src/main/java/ch04/Ch04Configuration.java`
[source,java]
----
include::../code/chapter04/src/main/java/ch04/Ch04Configuration.java[lines="22..61"]
----

Due to all the work we've done previously, this is all the configuration necessary to make a voice assistant like we've mapped out work.
Our next step is to write up a test for what we want to ask.
As we promised earlier, we'll show you some code inline that can be used if you want to record your own voice command.
For reference the configuration above has the light states as the following:

[cols="1,2"]
|===
| Light Name | Status

| yellow
| off

| red
| on

| green
| off

| blue
| on
|===

Let's take a look at a test which will send a "voice" command to our LLM asking it to "Turn on yellow light" and gets an spoken word audio file back in response.

.Listing {chapter}-{counter:listing}: `chapter04/src/test/java/ch04/VoiceAssistantTest.java`
[source,java]
----
include::../code/chapter04/src/test/java/ch04/VoiceAssistantTest.java[]
----

The test we crafted involves calling the `VoiceAssistantService` with text of "Turn on yellow light." and receives a byte array which is the MP3 audio file of the LLM response.
After receiving the byte array we assert that its not null and write the MP3 file to disk.

Now it's time to implement our somewhat convoluted `VoiceAssistantService` that our test calls to see this thing working.

.Listing {chapter}-{counter:listing}: `chapter04/src/main/java/ch04/service/VoiceAssistantService.java`
[source,java]
----
include::../code/chapter04/src/main/java/ch04/service/VoiceAssistantService.java[]
----

We've implemented two methods here for `issueCommand`.
The primary method takes a `Resource` which we expect to include the recorded speech command "Turn on yellow light", the other uses the `TextToSpeechService` in case you don't have a way of recording audio and passing it to the service.
For our test as written, it uses the method accepting a String to simplify the example.
The method transcribes the audio file with spoken words in it, passes it to the `UpdateChatService` for processing using the "real world" and then takes the LLM's text response and returns it as spoken word audio.

Let's re-run our tests for `chapter04`:

[source,shell]
----
mvn clean test -am -pl chapter04
----

If all worked as expected there should be an audio file saved on the filesystem named "voice_assistant_response.mp3" and playing it on your system it will contain a message like "The yellow light has been turned on."

As we indicated earlier, if you want to record your own audio command you can change the test to accommodate loading an MP3 file and sending that through the process.
Simply load your MP3 file (here we assume `voice_assistant_command.mp3` as a name) and pass that to the `issueCommand` method wrapped in a `ByteArrayResource`.

Something like the following.

[source,java]
----
        byte[] audioCommandAsBytes = Files.readAllBytes(Paths.get("./voice_assistant_command.mp3"));
        byte[] audioResponseBytes = voiceAssistantService.issueCommand(new ByteArrayResource(audioCommandAsBytes));
----

There you have it, a voice assistant that understands you.

== Next Steps

In our next chapter, we're going to look at image generation with Spring AI and object identification in images using the multi-modality API.
