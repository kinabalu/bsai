= Navigating AI in Engineering: Challenges and Best Practices
:chapter: 6

== A Practical Exploration of AI-Aided Development

In Chapter 1, we brought up a simple high-level overview of the AI landscape as it is today.
It's a useful chapter, not just because of the content it holds, but because of the way it was written.

It was drafted in Asciidoctorfootnote:[Asciidoctor (`https://asciidoctor.org`) is software for taking simple text content and generating a document model from it.] (as was the rest of the book), with the editor suggesting minor grammar changes as it was written.
These edits were focused on simple things like matching tenses, or spellingfootnote:[If you're interested, the tool used for grammar and syntax was Grammarly, at `https://app.grammarly.com/`, which is merely one of many such tools, and this is not an endorsement of Grammarly over other similar tools like ProWritingAid (`https://prowritingaid.com/`), and so forth. Most of them do the same sorts of things, although most of them _also_ tend to be more focused on specific types of writing; ProWritingAid, for example, is primarily meant for storytellers. Even Microsoft Word has similar grammar aids, like Copilot.].

After the initial simple draft was done and revised by the author, the content was submitted as a whole to an LLM (ChatGPT, specifically), with a prompt asking for hints toward readability, ease of use, and appropriate topical coverage.
ChatGPT then presented a potential rewrite of the chapter, which wasn't quite what was intended.

That rewritten draft was then considered sentence-by-sentence and compared to the original.
In some cases, the rewrite was indeed more clear, or highlighted issues in the original draft, and those changes were integrated into the chapter's content.
The changes were not copied or accepted wholesale.
Any original content suggested by ChatGPT was carefully considered before inclusion.

Only one section included new content suggested by ChatGPT, which was then rewritten before being added to the draft.
Apart from the quote from ChatGPT (on the "summary of AI"), no content was quoted from ChatGPT as original material.

The other chapters also utilized similar aids.
Grammar checkers were used throughout the writing process, and their suggestions were often accepted.
Additionally, the text was submitted to the LLM for evaluation to improve clarity and completeness.
The code, too, was evaluated by an LLM for suggestions for refactoring and efficiency, with some suggestions accepted and others rejected.

This text was written by humans, aided by AI, and not the other way aroundfootnote:[Of course, "written by humans and aided by AI, and not the other way around" is exactly what an AI author would be instructed to say, wouldn't it? The main proof we have that humans wrote this is in the revision history of the text, which includes some amusing and very human errors, and the silliness of some of the footnotes, which the AIs kept telling us to remove.].

While this is a _book_ and not a _program_, similar practices are emerging in programming.
AI suggests revisions to code, deriving intent from the programmer and the code they write based on code structure, name choices, and the like.
It can generate code based on prompts provided by the engineer, and it's up to the engineer to decide what to do with that code.

This is not without its dangers.

== Dangers in Applying AI in Engineering

If there's a concern around AI, it's not about a possible future in which an AI overthrows humanity: it's in how easy it is to mistake AI-generated content as being accurate.
Generating content with AI is easy and becoming easier all the time, and the AIs are getting better and better at generating realistic output, no matter what kind of medium is being used.

AI models depend heavily on the quality of their training data and are highly sensitive to data quality issues: "garbage in, garbage out" is very apt, and given the volume of data used to train even the smaller LLMs, it's very difficult to ensure that the data is correct; thus, it's relatively easy for an AI to confidently assert something that is very, very wrong.

AIs can generate working code relatively easily, and there are many AI models designed specifically for this purpose; asking an AI how to write a function to parse a given set of inputs, yielding a specific type of output, is likely to give you working code for whichever language you choose, and you can often even specify the parsing techniques.

For example, you could ask for the use of a packrat-style parserfootnote:[A "packrat-style parser" is a context-free parser. They tend to be quite fast, often very flexible, and their grammars can be a pain to write. See `https://en.wikipedia.org/wiki/Parsing_expression_grammar` for more details.] in Java, with examples of seven input lines, and show the data you want to extract.
Many of the AI models will generate full working code, including tests and suggestions of libraries.

The code will probablyfootnote:[Saying code with "probably" work should be offensive to competent programmers. Good programmers _know_ whether their code works. That's part of why this book was written to be test-heavy.] even work.
The danger here is that the AI will work _to your specification_, flaws and all, which means that the prompt needs to be as precise as possible, and even then, the generated code needs to be parsed carefully to make sure it *actually does what it's supposed to*.
It might be reliable, but you need to verify that.

You're not getting rid of the need for a competent engineer just because an AI can generate code.
Most experienced programmers can easily recount examples where a stakeholder described a complex process as "just process the order, it's obvious," only to find a complex problem lurking behind a simple request.

In the end, an AI is going to rely heavily on two things: your skill at specify a problem clearly and precisely, and your ability to evaluate whether the solution is appropriate.

There's no shortcut here.
Human expertise and critical thinking remain essential in effectively utilizing AI tools.

== Legal and Ethical Issues

AI also creates some interesting legal and ethical concerns.
Since AI creates content modeled on others' work, there's a danger of it being too similar to the original, potentially infringing on copyright laws.
This is without even considering the ethical implications of imitating someone else's work product.

For example, actors have filed lawsuits against some AI models' owners, claiming that their voices have been used by the AIs, without their permission.
We could theoretically - but not necessarily legally or ethically - have an AI replicate a famous actor's voice in a production, with no credit or compensation offered to that actor.

What's more, there are significant concerns that extend beyond copyright law.
There is a risk that AI could be misused to generate content that falsely represents real people, potentially damaging reputations, or spreading misinformation, or creating something that crosses the line from imitation into forgery.

Many of the models have been trained to attempt to avoid such violations; asking ChatGPT to create a painting in the style of Jackson Pollack, for example, creates a response that the image cannot be created due to content policy guidelines due to how uniquely identifiable Pollack's style is.
(Each model has its own policies here; ChatGPT is being used as an example only.)

With that said, developers have trained models without these ethical and legal guardrails: users should exercise caution and ethical consideration when employing these models.
They're not without purpose, but they should be approached with care.

If there are concerns about the legality of AI-generated content, it is advisable to consult a legal professional.

== Data Visibility and Transparency

While the previous section discussed the legality of data produced by AI, it's also important to consider the legal and privacy implications of the data you provide to an AI hosted by a third partyfootnote:[Local models offered by platforms like Ollama can offer enhanced data security compared to external services since they keep data on-premises. These models still should be checked to make sure they don't send information offsite, or isolated by firewalls to prevent possible breaches of security, but local models are *probably* safer than external services. They also require significant computational resources and can be slower or less efficient than cloud-based services. In any event, it's crucial for users to verify that even local models do not transmit data externally and to implement appropriate security measures to safeguard information.].

When using a third-party service like ChatGPT, as we do throughout this book, you're sending information outside of your direct control.
If your data is meant to be secure, submitting it to an external service can *violate* that security, because _you don't know what the service is doing with it_, a concern that goes beyond AI services, because you don't necessarily know what the services are doing with your data.

The service might log your data, and if those logs are hacked or exposed, your information could become compromised.

The service might use your contributed data to train future versions of its models, especially if it's operated by a social media conglomerate.
This means your private data could become accessible to anyone who formulates the right prompt to extract it.

The services themselves have a strong interest in preventing these scenarios, of course; it's unlikely that _any_ of the services are particularly interested in private or secured information in and of itself.
Most of them, if not all of them, will have documentation around their security and training practices, and this documentation should be read carefully.
Ultimately, it's up to users to ensure they understand the risks and take appropriate measures to safeguard their information.

NOTE: Depending on your locale, there may also be laws in place to govern what data you can send to external services, including but not limited to GDPRfootnote:[The text of the General Data Protection Regulation (GPDR) can be found at `https://gdpr-info.eu/`, and addresses the right and ownership of individual data in the European Union.] or HIPAAfootnote:[The Health Insurance Portability and Accountability Act (HIPAA) can be found at `https://www.hhs.gov/hipaa/index.html` and addresses patients' rights to privacy in the United States.].
Please do not violate any laws.

The rule of thumb might be: *If someone can use this data to identify the subject of the prompt, it might be too private to send to an AI.* However, this is a _generalized rule_ and shouldn't be taken as legal advice.

As with other legal and ethical concerns, a legal professional should be consulted if there's a question of whether data can be sent to an AI or not.

In summary, being mindful of the data you share with AI services is crucial to maintaining security and compliance with legal and ethical standards.

== Effective Prompt Engineering

While this book does not delve into the intricacies of prompt engineering—doing so would likely double its length—it is important to acknowledge that prompts can create responses that are biased or harmful, and it's our responsibility as users of AI to be aware of potential biases and and address such concerns accordingly.

To illustrate this point without delving into extensive detail, prompt writers should be aware of how prompts can change the responses, by embedding assumptions.

Assumptions in prompts might be entirely benign or innocuous: "Describe a teacher who loves her students," for example, _presumes_ that the teacher is female as part of the question. While this may be intentional — for instance, if focusing on a female teacher — it might also be an unintentional bias. A better query _might_ be "Describe a teacher who loves their students," using gender-neutral language, unless gender is _intended_ to be part of the response.

Even if a neutral question is asked, the response might be biased. A prompt asking for a description of a successful entrepreneur might get a consistent description of a male, for example, even though there are certainly successful female entrepreneurs.

These biases occur because AI models are trained on existing data, which may reflect societal biases. As a result, the AI might be more likely to produce responses that align with those biases.  If the models have more references to male business leaders than female entrepreneurs, then the models will themselves more likely describe entrepreneurs as male.

This reinforcement means that the common knowledge pool—which future models may use for training—contains even more references to successful male entrepreneurs. This creates a feedback loop, amplifying the bias without any malicious intent.

Models being trained on common knowledge also have to wrestle with the possibility that common knowledge is _wrong_. An engineer on LinkedIn had a rather appropriate observation:

[quote,Paul Parks,'https://www.linkedin.com/posts/paulmooreparks_ai-llm-activity-7240264719364673536-GpFJ/']
I'm going to be very wary of large-language models and AI in general until I find one that can say, "I don't know," when I ask a question about a technical matter. Generally, they behave like really eager interns that would rather make up an answer than admit to not knowing the answer.

Therefore, it's crucial for anyone interacting with AI systems to continually remain vigilant about these biases and actively work to mitigate them in pursuit of accuracy, honesty, and integrity.

== Next Steps

We've reached the end of the journey for _this_ particular book.

We've tried to focus on those elements from Spring AI that would prove most useful for most programmers, while acknowledging that there are a lot of use cases that go deeper in nearly everything we've touched.

Such is the nature of an introductory book. We'd like to invite you to explore and create using the technology, going as far as your imagination and skills can take you, and we wish you success in all that you do; show us what you've done, and tell us what we can do better!

Thank you.
