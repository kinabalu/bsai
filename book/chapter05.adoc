= Generating images
:chapter: 5

== Generating and Recognizing Images
The 1960s were an important time for the process of image recognition and generation similar to what we explored with audio in the previous chapter. Early image recognition using computer vision systems were focused on simple geometric shapes which evolved into techniques like edge detection and template matching. The complexity inherent in an image meant that these early techniques while impressive for their time were unable to deal with anything more than a very structured environment.

Over the years with advancements in usage of neural networks for learning-based approaches and later Support Vector Machines and other feature-based methods the object detection and classification had reached a well-defined commercial appeal especially with faster available hardware. Fast forward to the last 25 years and the development of Convolutional Neural Networks (CNNs) and advanced GPU processing has exploded the capabilities of image recognition. The techniques utilized in most of the chat models available via Spring AI use some variant of transformers like Vision Transformers to perform their work.

As of this writing the following offerings can perform image recognition via Spring AI:

* OpenAI (e.g. GPT-4 and GPT-4o models)
* Ollama (e.g. LlaVa, Baklava, Llama3.2 models)
* Vertex AI Gemini (e.g. gemini-1.5-pro-001, gemini-1.5-flash-001 models)
* Anthropic Claude 3


// TODO "basic generations" needs clarification
// TODO "in the last 20 years with the invention of GANS which used to neural networks to produce images..." this isn't actually a sentence. :)
// TODO I like this paragraph, but it needs editing
Generation has followed a somewhat similar history and progression over the last 60 years. The basic generations in the 1960s progressed into the 1980s and 1990s with 3D models, textures and realistic lighting using mathematically simulated scenes. In the last 20 years with the invention of Generative Adversarial Networks by Ian Goodfellow which used two neural networks in concert with each other to produce images that can look convincingly real. The real power has surfaced in the last few years with text-to-image generation models beginning with early models like DALL-E 1. Diffusion models were introduced commercially to the world in 2022 with releases by popular names now DALL-E 2, Midjourney and Stable Diffusion and was able to produce images that were more realistic and higher quality than ever before.

As of this writing the following are offered by Spring AI for image generation:

* OpenAI
* QianFan
* StabilityAI
* ZhiPuAI

---
* first thing is the setup
* second thing we'll do is set up and do some image generation. bowl of fruit maybe? and pass that to OpenAI and download the image
* third thing we'll do is take that image and pass it back through image recognition using CLIP



First task we'll do is to create our directory structure for this chapter. This can be created in the "project directory" as in previous chapters with the following command, if you're running a POSIX shell like `bash` or `zsh`:

.Listing {chapter}-{counter:listing}: Creating the project directory structure in POSIX
[source,shell]
----
mkdir -p src/{main,test}/{java,resources}
----

// TODO reference to webp is valuable, but "which is what OpenAI renders" would be a better footnote.
This is _this chapter's_ project file, and thus it's in a directory _under_ the top level directory, called `chapter05`, and the file is named `pom.xml`. A few items to note here, just as in chapter 4 we are going to include the `chapter03` module for a fun final challenge and a library extension for `ImageIO` which enables dealing with `webp` images which is what OpenAI renders. We have included a simple class which can write these files to disk and convert them to PNG before doing so.

.Listing {chapter}-{counter:listing}: `chapter05/pom.xml`
[source,xml]
----
include::../code/chapter05/pom.xml[]
----

Our application configuration will be automatic just like we've shown in previous chapters. Your OpenAI API key will again be fed into the property `spring.ai.openai.api-key`. Our configuration will also include getters for the yellow, red, green and blue light properties, and setting up for the services and functions made available from the `ch03` module to poll lights through ChatGPT and update their state.

.Listing {chapter}-{counter:listing}: `chapter05/src/main/java/ch05/Ch05Configuration.java`
[source,java]
----
include::../code/chapter05/src/main/java/ch05/Ch05Configuration.java[]
----


The `OpenAiImageApi` has a single request module `OpenAiImageRequest` which at a minimum would include
the prompt we'd use for instructing either the generation which invokes DALL-E or the recognition which
invokes CLIP. You will get a sense of the available options from the table below which can be included in properties or passed in with the request.

[cols="1,2"]
|===
| Property name
| Description

| `spring.ai.openai.api-key`
| The API key to be used by the application. Again for some reason this isn't utilized by `OpenAiImageApi` but we'll continue with this convention here.

| `spring.ai.openai.organization-id`
| Optionally specify the organization to use in the API request

| `spring.ai.openai.project-id`
| Optionally specify the project used for this API request
|===

The prefix spring.ai.openai.image is the property prefix that lets you configure the ImageModel implementation for OpenAI.

[cols="1,2"]
|===
| Property name
| Description

| `spring.ai.openai.image.enabled`
| Enable OpenAI image model (we don't know why you would want to disable this but there you go)

| `spring.ai.openai.image.options.n`
| Images to generate, for `dall-e-3` n=1 but can be up to 10 for `dall-e-2`

| `spring.ai.openai.image.options.model`
| The model to use for image generation. The `OpenAiImageApi.DEFAULT_IMAGE_MODEL` is `dall-e-3` but you can specify `dall-e-2` as well for speed and cost

| `spring.ai.openai.image.options.quality`
| Image quality by default will be `standard` but if using the `dall-e-3` model you can specify `hd` and the image will have finer details and greater consistency across the image

| `spring.ai.openai.image.options.response_format`
| The format in which the generated images are returned. Must be one of `URL` or `b64_json`.

| `spring.ai.openai.image.options.size`
| Size of generated images. Must be one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`. Must be one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3` models.

| `spring.ai.openai.image.options.size_width`
| Width of the generated images. Must be one of `256`, `512`, or `1024` for `dall-e-2`.

| `spring.ai.openai.image.options.size_height`
| Height of the generated images. Must be one of `256`, `512`, or `1024` for `dall-e-2`.

| `spring.ai.openai.image.options.style`
| Style of the generated images. Either `vivid` or `natural`. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This parameter is only supported for `dall-e-3`.

| `spring.ai.openai.image.options.user`
| A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
|===

This chapter similar to chapter 4 will include all tests which will perform executions against the OpenAI API. We will have the opportunity to go a bit deeper and use both the image generation and recognition pieces of the API in concert to create a silly but useful execution path utilizing Spring AI's image and multi-modal modules.

=== Image Generation

// TODO this opening sentence is a freaking honker.
Our first test case will be a simple prompt in practice these days but incredibly amazing when you think about the mountain of image data and technological achievement that is involved in taking a random set of text input and creating an image to represent it. At a super high level the text that is entered is tokenized with a transformer and converted into a high-dimensional embedding. That embedding is then mapped to a shared latent space where relationship between text and visual concepts are learned. The latent space is then passed through a generative diffusion model to create an image using the text embeddings and decoded into an image using CLIP contrastive learning model.

The text prompt we're using asks DALL-E to imagine a bowl of fruit containing bananas, apples and kiwis. Our code calls the `ImageGeneratorService` which passes back a base64 encoded image included in a JSON object.

.Listing {chapter}-{counter:listing}: `chapter05/src/test/java/ch05/GenerateImageTest.java`
[source,java]
----
include::../code/chapter05/src/test/java/ch05/GenerateImageTest.java[]
----

In a test first methodology we've now got something that will definitely not pass because it won't compile without an implementation of `ImageGeneratorService`. We'll get to the implementation of that in just a moment.

// TODO "today" is gonna feel dated when ".webp2" is what all the cool kids are using.
// TODO And we overuse "today" as a word.
// TODO Final sentence made me laugh. Good.
For now, the image that gets passed back, if you are interested in the output as part of the test the second half of the test writes the file to disk. OpenAI generated images are in `webp` format when they are sent down the wire. The `webp` format is the gold standard for web and mobile applications today due to it's more efficient file size and support in all modern browsers today. For saving and importing into a design tool though we can't help but love `png` for this so we've included a converter from `webp` to `png`.

.Listing {chapter}-{counter:listing}: `chapter05/src/main/java/ch05/WebpToPngConverter.java`
[source,java]
----
include::../code/chapter05/src/main/java/ch05/WebpToPngConverter.java[]
----

Now we get to the implementation of the `ImageGeneratorService` which has a singular and very simple method called `processPrompt(String, OpenAiImageOptions.Builder)` which takes a text prompt and an `OpenAiImageOptions` object which if null is passed we'll insert some defaults using the `application.properties` as specified.

With the text prompt given, we issue a call to the API we use `call()` to get the response back from the API which will interface with DALL-E and returns back the output of the `ImageResponse` returned.

Let's take a look at the actual class and what the implementation looks like.

// TODO I feel like the imageOptions has simpler code than a ternary op here
.Listing {chapter}-{counter:listing}: `chapter05/src/main/java/ch05/service/ImageGeneratorService.java`
[source,java]
----
include::../code/chapter05/src/main/java/ch05/service/ImageGeneratorService.java[]
----

Let's generate an image since our test will pass. We'll call the chapter's test suite using Maven:

[source,shell]
----
mvn -am -pl chapter05 clean test
----

// TODO it did not complete with SUCCESS. I got an error saying "rendered_fruit_bowl.png cannot be opened." It did, however, get created in the top level directory.
// TODO I would also include a sample image after this, showing what *your* run resulted in for readers. Theirs might not match, but it should communicate the idea. Bonus points if we include something odd in the image via prompt, like "with a praying mantis smiling at the viewer" or something like that.
If everything is working properly it should complete with `SUCCESS`.
After a successful run you can check the root of the projects filesystem and should see a new `png` image named `rendered_fruit_bowl.png` which will contain an image representation of what DALL-E thinks a fruit bowl looks like with bananas, apples and kiwis.

=== Multi-modality recognition

// TODO footnote is... uh... "nets out in the end"?
Now that we have an image that we have generated which we can use this in a test of the multi-modality integration. We'll again lean on using OpenAI as the source for our artificial intelligence needs and we can recyclefootnote:[Given the GPU cost of running a DALL-E generation perhaps reusing this image nets out in the end] the fruit bowl we just generated for our next test.

Our test will consist of two items that we'll be passing to the API: a prompt, and an image. For the image, we'll assume it is a `ClassPathResource` which means we need to copy it to `src/main/resources` which we can do like so:

// TODO AHHHH, this is why I got the error I received. Interesting. Okay, cool. This might want a footnote itself.
// TODO also, in the source from github, src/main/resources was NOT created. We may want to change this to src/test/resources.
[source,shell]
----
cp rendered_fruit_bowl.png src/main/resources
----

We'll take the `Resource` and the text prompt `In a single sentence explain what is in this picture.` and at the end do some simple assertions to ensure that the text description contains the words "banana", "apple" and "kiwi".

.Listing {chapter}-{counter:listing}: `chapter05/src/test/java/ch05/ImageRecognitionTest.java`
[source,java]
----
include::../code/chapter05/src/test/java/ch05/ImageRecognitionTest.java[]
----

One thing you'll notice is we are outputting the contents of the `recognition` object returned to stdout. We've left this in because during the testing of this book the hallucinations sometimes cause a test to fail and it helps to see why the assertions fail.

The other thing you'll see is that we can't quite run this yet since the `ImageRecognitionService` doesn't exist. Let's take a look at the implementation of `ImageRecognitionService` and see how the multi-modality integration of interrogating an image via a prompt is done.

.Listing {chapter}-{counter:listing}: `chapter05/src/main/java/ch05/service/ImageRecognitionService.java`
[source,java]
----
include::../code/chapter05/src/main/java/ch05/service/ImageRecognitionService.java[]
----

We have a `ChatModel` available in this class which we'll use Spring's dependency injection to reference in our `ImageRecognitionService`.

Our call to OpenAI is building a `UserMessage` which will contain the two items passed ni via the test: a text prompt, and the Media object. Our call to the API will be blocking so we'll use `call()` to get the response specification.

With our response we can pass back the description from OpenAI on what is inside the image.
For our usage and this test we don't have need for the metadata but just know that it's available if needed for your application.

Now that we have a service our tests should pass. We'll call the chapter's test suite using Maven:

[source,shell]
----
mvn -am -pl chapter05 clean test
----

// TODO I'd also like to see some sample output from maven here, for once, showing how OpenAI described the image.
If mercury isn't in retrograde in the OpenAI cloud, you should see `SUCCESS` in the test. The test itself will print out the returned text from OpenAI and the test itself should also pass.

One fun exercise you can try on your own is to expand your prompt to get more information about the media you upload to the API. We could imagine that uploading a PDF and asking it to summarize the contents could be another test of the multimodal nature of this module in Spring AI.

=== Lights, Camera, AI
Pur last test we're going to again re-use code from the lights in Chapter 3 and we're going to generate some images based on the lights properties we've defined in `Ch05Configuration` which we're letting OpenAI use as a data source.

Here's the process for what the following test actually does, and the great part about it is we're not doing anything new here so as soon as we explain ourselves here we can run the test right away.

We've got 3 lights that are possible in our system, and in the following test:

1. Ask the AI to create a prompt for DALL-E to generate an image of any lightbulb which is on.
2. Pass that prompt to the `ImageGeneratorService` and retrieve the `Image`
3. Pass that `Image` back to the AI and ask it to explain what's in the picture
4. Assert that at a minimum the explanation contains the word `red` since that's the light that should be turned on
5. Run a command with OpenAI to turn off the red light, and turn on the green light.
6. Re-run step 1 - 3 above again
7. Assert that at a minimum the explanation contains the word `green` now since we turned off the red light and turned on the green in our command.

Let's take a look at how this works in the test below.

.Listing {chapter}-{counter:listing}: `chapter05/src/test/java/ch05/LightVisualizerTest.java`
[source,java]
----
include::../code/chapter05/src/test/java/ch05/LightVisualizerTest.java[]
----

Now that we have the test and it reuses existing services, we'll again call the chapter's test suite using Maven:

[source,shell]
----
mvn -am -pl chapter05 clean test
----

Whew. We've got a test now performing the functions and it should pass. One could imagine if you hooked up a camera that captured an image at some regular frequency, and uploaded that capture to an AI you could ask it to perform a function based on what it saw in the photo. Pretty neat.

== Next Steps

In our next chapter, we'll wrap up with a discussion on navigating AI in engineering, the challenges and share some best practices.
