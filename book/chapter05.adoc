= Generating images
:chapter: 5

== Generating and Recognizing Images
The 1960s were an important time for the process of image recognition and generation similar to what we explored with audio in the previous chapter. Early image recognition using computer vision systems were focused on simple geometric shapes which evolved into techniques like edge detection and template matching. The complexity inherent in an image meant that these early techniques while impressive for their time were unable to deal with anything more than a very structured environment.

Over the years with advancements in usage of neural networks for learning-based approaches and later Support Vector Machines and other feature-based methods the object detection and classification had reached a well-defined commercial appeal especially with faster available hardware. Fast forward to the last 25 years and the development of Convolutional Neural Networks (CNNs) and advanced GPU processing has exploded the capabilities of image recognition. The techniques utilized in most of the chat models available via Spring AI use some variant of transformers like Vision Transformers to perform their work.

As of this writing the following offerings can perform image recognition via Spring AI:

* OpenAI (e.g. GPT-4 and GPT-4o models)
* Ollama (e.g. LlaVa, Baklava, Llama3.2 models)
* Vertex AI Gemini (e.g. gemini-1.5-pro-001, gemini-1.5-flash-001 models)
* Anthropic Claude 3

Generation has followed a somewhat similar history and progression over the last 60 years. The basic generations in the 1960s progressed into the 1980s and 1990s with 3D models, textures and realistic lighting using mathematically simulated scenes. In the last 20 years with the invention of Generative Adversarial Networks by Ian Goodfellow which used two neural networks in concert with each other to produce images that can look convincingly real. The real power has surfaced in the last few years with text-to-image generation models beginning with early models like DALL-E 1. Diffusion models were introduced commercially to the world in 2022 with releases by popular names now DALL-E 2, Midjourney and Stable Diffusion and was able to produce images that were more realistic and higher quality than ever before.

As of this writing the following are offered by Spring AI for image generation:

* OpenAI
* QianFan
* StabilityAI
* ZhiPuAI

---
* first thing is the setup
* second thing we'll do is set up and do some image generation. bowl of fruit maybe? and pass that to OpenAI and download the image
* third thing we'll do is take that image and pass it back through image recognition using CLIP



First task we'll do is to create our directory structure for this chapter. This can be created in the "project directory" as in previous chapters with the following command, if you're running a POSIX shell like `bash` or `zsh`:

.Listing {chapter}-{counter:listing}: Creating the project directory structure in POSIX
[source,shell]
----
mkdir -p src/{main,test}/{java,resources}
----

This is _this chapter's_ project file, and thus it's in a directory _under_ the top level directory, called `chapter05`, and the file is named `pom.xml`.

.Listing {chapter}-{counter:listing}: `chapter05/pom.xml`
[source,xml]
----
include::../code/chapter05/pom.xml[]
----

Creating an `OpenAiImageApi` is somewhat of a manual process

The `OpenAiImageApi` has three main request modules that it enables for working with audio.
The first we'll work with is the `OpenAiImageRequest` which may just be the text we want to create an audio file but can also accept several other options which we'll go over below.

As with other modules in Spring AI they can be controlled at the point of the request, in code, or from the application propertiesfootnote:[We consider it a bug that these properties seem to be ignored by the current version of the API.].

[cols="1,2"]
|===
| Property name
| Description

| `spring.ai.openai.api-key`
| The API key to be used by the application. Again for some reason this isn't utilized by `OpenAiImageApi` but we'll continue with this convention here.

| `spring.ai.openai.organization-id`
| Optionally specify the organization to use in the API request

| `spring.ai.openai.project-id`
| Optionally specify the project used for this API request
|===

The prefix spring.ai.openai.image is the property prefix that lets you configure the ImageModel implementation for OpenAI.

[cols="1,2"]
|===
| Property name
| Description

| `spring.ai.openai.image.enabled`
| Enable OpenAI image model (we don't know why you would want to disable this but there you go)

| `spring.ai.openai.image.options.n`
| Images to generate, for `dall-e-3` n=1 but can be up to 10 for `dall-e-2`

| `spring.ai.openai.image.options.model`
| The model to use for image generation. The `OpenAiImageApi.DEFAULT_IMAGE_MODEL` is `dall-e-3` but you can specify `dall-e-2` as well for speed and cost

| `spring.ai.openai.image.options.quality`
| Image quality by default will be `standard` but if using the `dall-e-3` model you can specify `hd` and the image will have finer details and greater consistency across the image

| `spring.ai.openai.image.options.response_format`
| The format in which the generated images are returned. Must be one of `URL` or `b64_json`.

| `spring.ai.openai.image.options.size`
| Size of generated images. Must be one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`. Must be one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3` models.

| `spring.ai.openai.image.options.size_width`
| Width of the generated images. Must be one of `256`, `512`, or `1024` for `dall-e-2`.

| `spring.ai.openai.image.options.size_height`
| Height of the generated images. Must be one of `256`, `512`, or `1024` for `dall-e-2`.

| `spring.ai.openai.image.options.style`
| Style of the generated images. Either `vivid` or `natural`. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This parameter is only supported for `dall-e-3`.

| `spring.ai.openai.image.options.user`
| A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
|===

wha

=== Multimodality
Okay, so what are the other nice things about this? I would say so the multi-modality aspect of some of the large language models out there mean that we can understand what is contained within So a mode could be an image or an audio file and when we understand more about what's there, it means we can take an image that we got off of the web Let's say and ask it for the text of what's in that object and possibly convert that text to a different language, let's say The other thing we can also do is take an image, a photo of something and ask the large language model what's in this photo identify it for me, describe for me what's in this photo or another example could be uploading an audio file that we would like to find out what it is Maybe it's a piece of music and given a substantially large model like ChatGPT 4.0, it's possible that we can upload a piece of music and it could tell you what that music is akin to what Shazam does and not only that, it could tell you similar music to that or it will allow you to upload multiple thousands of these music files and allow you to identify what they are, what type of music it is, which would ordinarily take days of a human's time but the AI can do it fairly quickly So that has some value


=== Optical Character Recognition
One thing that has been made easier and enhanced with large language models is processing images to find the text within.


https://upload.wikimedia.org/wikipedia/commons/8/8b/429-460_Police_Interceptor.jpg


=== Understanding an image


== Next Steps

In our next chapter, ...